{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping del diario La Nacion\n",
    "## Solo titulares entre fechas predeterminadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todo lo necesario\n",
    "# -- coding: utf-8 --\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from time import sleep\n",
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from sklearn.feature_extraction import text \n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "import locale\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "import datetime\n",
    "# debbuging...\n",
    "import pdb\n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechas que vamos a analizar\n",
    "# 2006 09 10\n",
    "# 2016 02 18\n",
    "\n",
    "# Seteamos el LOCALE a español (por las fechas...)\n",
    "locale.setlocale(locale.LC_TIME, \"es_AR.utf8\")\n",
    "\n",
    "# El buscador de lanacion pagina de a 10 los resultados\n",
    "# En el url se colocan las fechas de inicio y final\n",
    "# y las ahcemos coincidir con las que tenemos de perfil\n",
    "# por ultimo se ordena de viejoa nuevo y se elige la page del paginado\n",
    "# segun el indice, hay 477 paginas\n",
    "url_page = \"https://buscar.lanacion.com.ar/ediciones anteriores/date-20060910,20160218/sort-old/page-\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_csv(fila):\n",
    "    \"\"\" Recibe la fila y lo graba en el archivo tit_perfil.csv\n",
    "    \"\"\"\n",
    "    with open('tit_lanacion.csv', 'a') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerow(fila)\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pagina in range(1,478):\n",
    "#for pagina in range(1,3):\n",
    "    # cargar la pagina\n",
    "    # despejar todos los li.floatFix  \n",
    "    # recorrerlos\n",
    "    # despejar el spam class=\"tituloNota\" h2 que es el titular\n",
    "    # luego el p que contiene la entradilla\n",
    "    # Del div class=\"data\" sacamos la fecha en formato 16 OCT 2005\n",
    "    # y del primer a de ese div obtenemos la seccion del articulo\n",
    "        url = url_page + str(pagina)\n",
    "        #print(url)\n",
    "        req = requests.get(url)\n",
    "        # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "        status_code = req.status_code\n",
    "        if status_code == 200:\n",
    "            soup = BeautifulSoup(req.content, \"lxml\")\n",
    "            for li in soup.find_all('li', attrs={'class': 'floatFix'}):\n",
    "                #print(li)\n",
    "                titulo = li.find('span', {'class': 'tituloNota'}).h2.getText().lower()\n",
    "                #print(titulo)\n",
    "                fecha_txt = li.find('div',{'class': 'data'}).getText()[:11]\n",
    "                fecha_object = datetime.datetime.strptime(fecha_txt, '%d %b %Y').date()\n",
    "                anno =fecha_object.year\n",
    "                mes = fecha_object.month\n",
    "                dia = fecha_object.day\n",
    "                seccion = li.find('div',{'class': 'data'}).a.getText().lower()\n",
    "                #print(seccion)\n",
    "                entradilla = li.find('p').getText().lower()\n",
    "                #print(entradilla)\n",
    "                fila = [anno, mes, dia, seccion, titulo, entradilla]\n",
    "                add_to_csv(fila)\n",
    "                sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_object = datetime.datetime.strptime('01 FEB 2006', '%d %b %Y').date()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
