{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio Procesamiento del Lenguaje Natural\n",
    "## NLP - www.ApendeMachineLearning.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizaremos los cuentos del escritor Hernán Casciari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sus contenidos están en Español y son libres. (también puedes comprar sus libros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargaremos los textos de su Blog con cuentos de humor de los años 2004 a 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizaremos su obra para ver comprender sobre lo que escribe y su evolución a lo largo del tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes visitar su blog y cuentos en hernancasciari.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuestra Agenda será\n",
    "<ul><li>1 - Obtener datos</li>\n",
    "    <li>2 - Cargar los datos</li>\n",
    "    <li>3 - Limpiar datos </li>\n",
    "    <li>4 - Analisis Exploratorio</li>\n",
    "    <li>5 - Anáisis de Sentimiento</li>\n",
    "    <li>6 - Modelado de Temáticas</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T11:20:40.801379Z",
     "start_time": "2019-01-13T11:20:38.391146Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Obtener los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T11:37:34.658135Z",
     "start_time": "2019-01-05T11:37:34.650415Z"
    }
   },
   "outputs": [],
   "source": [
    "def url_to_transcript(url):\n",
    "    '''Obtener los enlaces del blog de Hernan Casciari.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    print('URL',url)\n",
    "    enlaces = []\n",
    "    for title in soup.find_all(class_=\"entry-title\"):\n",
    "        for a in title.find_all('a', href=True):\n",
    "            print(\"Found link:\", a['href'])\n",
    "            enlaces.append(a['href'])\n",
    "    sleep(0.75) #damos tiempo para que no nos penalice un firewall\n",
    "    return enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T09:05:57.881405Z",
     "start_time": "2019-01-08T09:05:57.788397Z"
    }
   },
   "outputs": [],
   "source": [
    "base = 'https://editorialorsai.com/category/epocas/'\n",
    "urls = []\n",
    "anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "for anio in anios:\n",
    "    urls.append(base + anio + \"/\")\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T11:39:00.268532Z",
     "start_time": "2019-01-05T11:37:38.005682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recorrer las URLs y obtener los enlaces\n",
    "enlaces = [url_to_transcript(u) for u in urls]\n",
    "print(enlaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T11:39:00.278281Z",
     "start_time": "2019-01-05T11:39:00.272445Z"
    }
   },
   "outputs": [],
   "source": [
    "def url_get_text(url):\n",
    "    '''Obtener los textos de los cuentos de Hernan Casciari.'''\n",
    "    print('URL',url)\n",
    "    text=\"\"\n",
    "    try:\n",
    "        page = requests.get(url).text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        text = [p.text for p in soup.find(class_=\"entry-content\").find_all('p')]\n",
    "    except Exception:\n",
    "        print('ERROR, puede que un firewall nos bloquea.')\n",
    "        return ''\n",
    "    sleep(0.75) #damos tiempo para que no nos penalice un firewall\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T13:08:15.060225Z",
     "start_time": "2019-01-05T12:25:06.654490Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recorrer las URLs y obtener los textos\n",
    "MAX_POR_ANIO = 50 # para no saturar el server\n",
    "textos=[]\n",
    "for i in range(len(anios)):\n",
    "    arts = enlaces[i]\n",
    "    arts = arts[0:MAX_POR_ANIO]\n",
    "    textos.append([url_get_text(u) for u in arts])\n",
    "print(len(textos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:52:00.825294Z",
     "start_time": "2019-01-04T15:52:00.805580Z"
    }
   },
   "outputs": [],
   "source": [
    "#Probamos a ver alguno de los textos\n",
    "print(len(textos[0]))\n",
    "print(textos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T13:08:15.253722Z",
     "start_time": "2019-01-05T13:08:15.063588Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pickle files para usar luego\n",
    "\n",
    "# # Creamos un directorio y nombramos los archivos por año\n",
    "!mkdir blog\n",
    "\n",
    "for i, c in enumerate(anios):\n",
    "    with open(\"blog/\" + c + \".txt\", \"wb\") as file:\n",
    "        cad=\"\"\n",
    "        for texto in textos[i]:\n",
    "            for texto0 in texto:\n",
    "                cad=cad + texto0\n",
    "        pickle.dump(cad, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Cargar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.219894Z",
     "start_time": "2019-01-11T20:17:09.873242Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Cargamos los pickled files\n",
    "anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "data = {}\n",
    "for i, c in enumerate(anios):\n",
    "    with open(\"blog/\" + c + \".txt\", \"rb\") as file:\n",
    "        data[c] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.230510Z",
     "start_time": "2019-01-11T20:17:10.222938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Revisamos que se haya guardado bien\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.240556Z",
     "start_time": "2019-01-11T20:17:10.233716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Veamos algun trozo de texto\n",
    "data['2008'][1000:1222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.251440Z",
     "start_time": "2019-01-11T20:17:10.244054Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkeamos primer clave\n",
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.267231Z",
     "start_time": "2019-01-11T20:17:10.254548Z"
    }
   },
   "outputs": [],
   "source": [
    "# nuestro diccionario esta cómo clave:Año valor:texto\n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.279653Z",
     "start_time": "2019-01-11T20:17:10.270406Z"
    }
   },
   "outputs": [],
   "source": [
    "# lo combinamos\n",
    "data_combined = {key: [value] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.327857Z",
     "start_time": "2019-01-11T20:17:10.283975Z"
    }
   },
   "outputs": [],
   "source": [
    "# lo metemos en un Panda's dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['transcript']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.343736Z",
     "start_time": "2019-01-11T20:17:10.331015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Veamos uno de los contenidos\n",
    "data_df.transcript.loc['2007']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Limpiar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.357312Z",
     "start_time": "2019-01-11T20:17:10.346741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aplicaremos varios rounds de limpieza\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.819636Z",
     "start_time": "2019-01-11T20:17:10.360844Z"
    }
   },
   "outputs": [],
   "source": [
    "# vemos la primer limpieza\n",
    "data_clean = pd.DataFrame(data_df.transcript.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.826960Z",
     "start_time": "2019-01-11T20:17:10.822347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Segundo round\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…«»]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.873740Z",
     "start_time": "2019-01-11T20:17:10.829538Z"
    }
   },
   "outputs": [],
   "source": [
    "# veamos como queda\n",
    "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.880285Z",
     "start_time": "2019-01-11T20:17:10.876876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at our dataframe\n",
    "#data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:10.893481Z",
     "start_time": "2019-01-11T20:17:10.884093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Como no tenemos un Lemmatizer en español, hacemos manualmente algunas conversiones\n",
    "# OJO: esto realmente no se hace a mano!!!\n",
    "\n",
    "def detectadas(palabra):\n",
    "    eliminar_s = ('libreros','textos','papelitos','monedas','páginas','anécdotas','perros','cuadernos','blogs',\n",
    "                  'revistas','caballos','vecinos','madres','puntos','ricos','libros')\n",
    "    if palabra in eliminar_s :\n",
    "        return palabra[:-1]\n",
    "    eliminar_es = ('mundiales','lectores','campeones','maníes','ustedes','autores')\n",
    "    if palabra in eliminar_es:\n",
    "        return palabra[:-2]\n",
    "    return palabra\n",
    "\n",
    "def clean_text_round3(text):\n",
    "    '''.'''\n",
    "    return \" \".join([detectadas(word) for word in text.split()])\n",
    "    \n",
    "round3 = lambda x: clean_text_round3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:11.215931Z",
     "start_time": "2019-01-11T20:17:10.896756Z"
    }
   },
   "outputs": [],
   "source": [
    "#vemos como queda\n",
    "data_clean = pd.DataFrame(data_clean.transcript.apply(round3))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:11.251397Z",
     "start_time": "2019-01-11T20:17:11.218209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Esto es un nuevo campo por si quisieramos agregar alguna info adicional a cada año\n",
    "# Nuestro caso repetimos los años, nos servirá para alguna visualización\n",
    "full_names = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "\n",
    "data_df['full_name'] = full_names\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:11.279343Z",
     "start_time": "2019-01-11T20:17:11.254053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hacemos el pickle para usar más adelante\n",
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:11.296292Z",
     "start_time": "2019-01-11T20:17:11.281918Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clean.transcript[0:255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:11.829023Z",
     "start_time": "2019-01-11T20:17:11.299422Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common Spanish stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "with open('spanish.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "cv = CountVectorizer(stop_words=lines)\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:11.868184Z",
     "start_time": "2019-01-11T20:17:11.832230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lo guardamos como pickle\n",
    "data_dtm.to_pickle(\"dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:12.018153Z",
     "start_time": "2019-01-11T20:17:11.873645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lo guardamos como pickle también\n",
    "data_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Análisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:12.064166Z",
     "start_time": "2019-01-11T20:17:12.020405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the document-term matrix\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle('dtm.pkl')\n",
    "data = data.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:12.141493Z",
     "start_time": "2019-01-11T20:17:12.066605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the top 30 words (per Year)\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:12.151238Z",
     "start_time": "2019-01-11T20:17:12.144805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the top 15 words p/Year\n",
    "for anio, top_words in top_dict.items():\n",
    "    print(anio)\n",
    "    print(', '.join([word for word, count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:12.165331Z",
     "start_time": "2019-01-11T20:17:12.154315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each anio\n",
    "words = []\n",
    "for anio in data.columns:\n",
    "    top = [word for (word, count) in top_dict[anio]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:12.183627Z",
     "start_time": "2019-01-11T20:17:12.170034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's aggregate this list and identify the most common words along with how many routines they occur in\n",
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:12.202924Z",
     "start_time": "2019-01-11T20:17:12.187257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Las mas repetidas las descartaremos\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:13.583211Z",
     "start_time": "2019-01-11T20:17:12.205904Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/Users/jbagnato/python_projects/blog' \n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
    "#wordlists.fileids()\n",
    "#pals = wordlists.words('2004.txt')\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "        (word,genre)\n",
    "        for genre in anios\n",
    "        for w in wordlists.words(genre + '.txt')\n",
    "        for word in ['casa','mundo','tiempo','vida']\n",
    "        if w.lower().startswith(word) )\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:14.207563Z",
     "start_time": "2019-01-11T20:17:13.585779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "with open('spanish.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "for pal in add_stop_words:\n",
    "    stop_words.append(pal)\n",
    "more_stop_words=['alex','andrés','asi','andres','así','año','alejandro','alfonso','allí','alguien',\n",
    "                 'basdala','bernardo','bien',\n",
    "                 'cosa','cosas','costoya','costa','cinco','celoni','cuatro','cómo','casi','colo','caprio','českomoravský','české','costa','canoso','carla','comequechu',\n",
    "                 'dos','dice','decir','días','dije','digo','diez',\n",
    "                 'ésa', 'ésas', 'ése', 'ésos', 'ésta', 'éstas', 'éste', 'ésto', 'éstos',\n",
    "                 'fernando','fenwick',\n",
    "                 'gelós','gente',\n",
    "                 'hornby','hernan','hernán','hoy','horacio','horas','hará','hans','hacía','haber',\n",
    "                 'iveta',\n",
    "                 'jesús','jorge','juan',\n",
    "                 'karen',\n",
    "                 'lucas','luego', 'luis',\n",
    "                 'mirta','mientras','menos','mónica','medio','mil','moncho','momento','mañana','mejor',\n",
    "                 'narcís','número','noche','nadie',\n",
    "                 'ojos',\n",
    "                 'primer','primera','pase','pablo','pepe','pack','peter', 'pues','prieto','politto','pol','paola','puede','próximo','podrán','podía',\n",
    "                 'quizá','quizás','quince','quién','quiero',\n",
    "                 'rato',\n",
    "                 'sólo','solamente','sakhan','šeredova','seis','šeredovà','seselovsky','solo','salas','sant','sino','se','sé','sabés','semana','soto','sido','solamente',\n",
    "                 'tres','tan','todas','trece','toda','todavía','tarde','tener',\n",
    "                 'uno','usted',\n",
    "                 'veces','ver','ve','vos','va','voy',\n",
    "                 'waiser','woung'\n",
    "                ]\n",
    "for pal in more_stop_words:\n",
    "    stop_words.append(pal)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.index\n",
    "\n",
    "# Pickle it for later use\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:14.217707Z",
     "start_time": "2019-01-11T20:17:14.210335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:20.666437Z",
     "start_time": "2019-01-11T20:17:14.221876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the output dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16,12]\n",
    "\n",
    "anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "\n",
    "# Create subplots for each anio\n",
    "for index, anio in enumerate(data.columns):\n",
    "    wc.generate(data_clean.transcript[anio])\n",
    "    plt.subplot(4, 3, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(anios[index])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:20.682268Z",
     "start_time": "2019-01-11T20:17:20.668841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the number of unique words per Year\n",
    "\n",
    "# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n",
    "unique_list = []\n",
    "for anio in data.columns:\n",
    "    uniques = data[anio].nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "\n",
    "# Create a new dataframe that contains this unique word count\n",
    "data_words = pd.DataFrame(list(zip(anios, unique_list)), columns=['Anio', 'unique_words'])\n",
    "#data_unique_sort = data_words.sort_values(by='unique_words')\n",
    "data_unique_sort = data_words # sin ordenar\n",
    "data_unique_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:20.698647Z",
     "start_time": "2019-01-11T20:17:20.686808Z"
    }
   },
   "outputs": [],
   "source": [
    "# ejecuta este si hicimos el webscrapping, o no tenemos los valores en la variable\n",
    "posts_per_year=[]\n",
    "try:\n",
    "  enlaces\n",
    "except NameError:\n",
    "  # Si no hice, los tengo hardcodeados:\n",
    "    posts_per_year = [50, 27, 18, 50, 42, 22, 50, 33, 31, 17, 33, 13]\n",
    "else:\n",
    "    for i in range(len(anios)):\n",
    "        arts = enlaces[i]\n",
    "        #arts = arts[0:10] #limito a maximo 10 por año\n",
    "        print(anios[i],len(arts))\n",
    "        posts_per_year.append(min(len(arts),MAX_POR_ANIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:20.737864Z",
     "start_time": "2019-01-11T20:17:20.702365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the words per post of each Year\n",
    "\n",
    "# Find the total number of words per Year\n",
    "total_list = []\n",
    "for anio in data.columns:\n",
    "    totals = sum(data[anio])\n",
    "    total_list.append(totals)\n",
    "    \n",
    "# Let's add some columns to our dataframe\n",
    "data_words['total_words'] = total_list\n",
    "data_words['posts_per_year'] = posts_per_year\n",
    "data_words['words_per_posts'] = data_words['total_words'] / data_words['posts_per_year']\n",
    "\n",
    "# Sort the dataframe by words per minute to see who talks the slowest and fastest\n",
    "#data_wpm_sort = data_words.sort_values(by='words_per_posts')\n",
    "data_wpm_sort = data_words #sin ordenar\n",
    "data_wpm_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:21.303139Z",
     "start_time": "2019-01-11T20:17:20.741071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot our findings\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "y_pos = np.arange(len(data_words))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(y_pos,posts_per_year, align='center')\n",
    "plt.yticks(y_pos, anios)\n",
    "plt.title('Number of Posts', fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(y_pos, data_unique_sort.unique_words, align='center')\n",
    "plt.yticks(y_pos, data_unique_sort.Anio)\n",
    "plt.title('Number of Unique Words', fontsize=20)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(y_pos, data_wpm_sort.words_per_posts, align='center')\n",
    "plt.yticks(y_pos, data_wpm_sort.Anio)\n",
    "plt.title('Number of Words Per Posts', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Análisis de Sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:17:21.406385Z",
     "start_time": "2019-01-11T20:17:21.305625Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Leeremos el corpus que aún preserva el orden de las palabras\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle('corpus.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:12.412110Z",
     "start_time": "2019-01-11T20:17:21.409174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create quick lambda functions to find the polarity and subjectivity of each routine\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n",
    "from textblob import TextBlob\n",
    "    \n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "pol2 = lambda x: x.sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "sub2 = lambda x: x.sentiment.subjectivity\n",
    "\n",
    "# Realmente lo traducimos al inglés pues el analisis de sentimiento de TextBlob no funciona en Español :(\n",
    "traducir = lambda x: TextBlob(x).translate(to=\"en\")\n",
    "\n",
    "data['blob_en'] = data['transcript'].apply(traducir)\n",
    "data['polarity'] = data['blob_en'].apply(pol2)\n",
    "data['subjectivity'] = data['blob_en'].apply(sub2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:12.817545Z",
     "start_time": "2019-01-11T20:18:12.414745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for index, anio in enumerate(data.index):\n",
    "    x = data.polarity.loc[anio]\n",
    "    y = data.subjectivity.loc[anio]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)\n",
    "    plt.xlim(-0.051, 0.152) \n",
    "    \n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
    "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:12.826920Z",
     "start_time": "2019-01-11T20:18:12.821606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split each routine into 12 parts\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def split_text(text, n=12):\n",
    "    '''Takes in a string of text and splits into n equal parts, with a default of 12 equal parts.'''\n",
    "\n",
    "    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n",
    "    length = len(text)\n",
    "    size = math.floor(length / n)\n",
    "    start = np.arange(0, length, size)\n",
    "    \n",
    "    # Pull out equally sized pieces of text and put it into a list\n",
    "    split_list = []\n",
    "    for piece in range(n):\n",
    "        split_list.append(text[start[piece]:start[piece]+size])\n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:12.892044Z",
     "start_time": "2019-01-11T20:18:12.833301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:12.967895Z",
     "start_time": "2019-01-11T20:18:12.895474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a list to hold all of the pieces of text\n",
    "list_pieces = []\n",
    "for t in data.blob_en:#transcript:\n",
    "    split = split_text(t,12)\n",
    "    list_pieces.append(split)   \n",
    "#list_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:12.978110Z",
     "start_time": "2019-01-11T20:18:12.971027Z"
    }
   },
   "outputs": [],
   "source": [
    "# The list has n elements, one for each transcript\n",
    "len(list_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:12.987841Z",
     "start_time": "2019-01-11T20:18:12.981418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each transcript has been split into 10 pieces of text\n",
    "len(list_pieces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:15.572897Z",
     "start_time": "2019-01-11T20:18:12.994742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the polarity for each piece of text\n",
    "\n",
    "polarity_transcript = []\n",
    "for lp in list_pieces:\n",
    "    polarity_piece = []\n",
    "    for p in lp:\n",
    "        #polarity_piece.append(TextBlob(p).translate(to=\"en\").sentiment.polarity)\n",
    "        polarity_piece.append(p.sentiment.polarity)\n",
    "    polarity_transcript.append(polarity_piece)\n",
    "    \n",
    "polarity_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:15.771231Z",
     "start_time": "2019-01-11T20:18:15.576699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show the plot for one anio\n",
    "plt.plot(polarity_transcript[0])\n",
    "plt.title(data['full_name'].index[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:17.628325Z",
     "start_time": "2019-01-11T20:18:15.774426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show the plot for all anios\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "\n",
    "for index, anio in enumerate(data.index):    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.plot(polarity_transcript[index])\n",
    "    plt.plot(np.arange(0,12), np.zeros(12))\n",
    "    plt.title(data['full_name'][index])\n",
    "    plt.ylim(ymin=-.45, ymax=.45)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Modelado de Temáticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos diversos intentos para obtener los temas que predominan en los cuentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:17.742801Z",
     "start_time": "2019-01-11T20:18:17.631721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:17.751018Z",
     "start_time": "2019-01-11T20:18:17.746104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:17.773299Z",
     "start_time": "2019-01-11T20:18:17.754772Z"
    }
   },
   "outputs": [],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:17.792166Z",
     "start_time": "2019-01-11T20:18:17.776607Z"
    }
   },
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:17.877889Z",
     "start_time": "2019-01-11T20:18:17.794951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:27.103670Z",
     "start_time": "2019-01-11T20:18:17.880590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:38.123165Z",
     "start_time": "2019-01-11T20:18:27.106700Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:18:49.856250Z",
     "start_time": "2019-01-11T20:18:38.126991Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 2: sólo Sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:02.809096Z",
     "start_time": "2019-01-11T20:30:02.803849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text,language='spanish')\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:02.867854Z",
     "start_time": "2019-01-11T20:30:02.813502Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:02.887101Z",
     "start_time": "2019-01-11T20:30:02.871609Z"
    }
   },
   "outputs": [],
   "source": [
    "colname=[]\n",
    "list_pieces = []\n",
    "contador=0\n",
    "for t in data_clean.transcript:\n",
    "    split = split_text(t,posts_per_year[contador]-7)\n",
    "    subcont=0\n",
    "    for p in split:\n",
    "        list_pieces.append(p)\n",
    "        colname.append(str(2004+contador)+ \"-\" + str(subcont))\n",
    "        subcont=subcont+1\n",
    "    contador=contador+1\n",
    "len(list_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:02.916906Z",
     "start_time": "2019-01-11T20:30:02.890693Z"
    }
   },
   "outputs": [],
   "source": [
    "data_split = pd.DataFrame(data=list_pieces).transpose()\n",
    "data_split.columns=colname\n",
    "data_split2=data_split.transpose()\n",
    "data_split2.columns = ['transcript']\n",
    "data_split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:31.195181Z",
     "start_time": "2019-01-11T20:30:02.920177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_split2.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:31.553515Z",
     "start_time": "2019-01-11T20:30:31.199177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "with open('spanish.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "for pal in add_stop_words:\n",
    "    stop_words.append(pal)\n",
    "for pal in more_stop_words:\n",
    "    stop_words.append(pal)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:31.666753Z",
     "start_time": "2019-01-11T20:30:31.556947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:46.232086Z",
     "start_time": "2019-01-11T20:30:31.669321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:30:59.701278Z",
     "start_time": "2019-01-11T20:30:46.235267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:31:12.555814Z",
     "start_time": "2019-01-11T20:30:59.704151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try topics = 4\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 3: Sustantivos y adjetivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:31:12.563796Z",
     "start_time": "2019-01-11T20:31:12.558591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text,language='spanish')\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:31:40.917668Z",
     "start_time": "2019-01-11T20:31:12.566879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_split2.transcript.apply(nouns_adj)) #data_clean\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:31:41.359892Z",
     "start_time": "2019-01-11T20:31:40.920733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:31:41.368860Z",
     "start_time": "2019-01-11T20:31:41.363294Z"
    }
   },
   "outputs": [],
   "source": [
    "#data_dtmna['escritor']\n",
    "print(data_dtmna.shape)\n",
    "#print(cvna.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:31:41.472013Z",
     "start_time": "2019-01-11T20:31:41.371644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:31:57.109909Z",
     "start_time": "2019-01-11T20:31:41.474891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:32:13.453790Z",
     "start_time": "2019-01-11T20:31:57.113075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's start with 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:32:34.897529Z",
     "start_time": "2019-01-11T20:32:13.456862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probamos a modelar con 4 tópicos\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificar los temas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:36:54.416868Z",
     "start_time": "2019-01-11T20:35:46.163832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our final LDA model\n",
    "QTY_TOPICS=4\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=QTY_TOPICS, id2word=id2wordna, passes=40,\n",
    "                        random_state=15)\n",
    "ldana.print_topics(QTY_TOPICS,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:34:53.530507Z",
     "start_time": "2019-01-11T20:34:41.985654Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/Users/jbagnato/python_projects/blog' \n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
    "#wordlists.fileids()\n",
    "#pals = wordlists.words('2004.txt')\n",
    "for i in range(QTY_TOPICS):\n",
    "    theList=ldana.get_topic_terms(i)\n",
    "\n",
    "    cfd = nltk.ConditionalFreqDist(\n",
    "        (word,genre)\n",
    "        for genre in anios\n",
    "        for w in wordlists.words(genre + '.txt')\n",
    "        for word in [id2wordna.get(a) for (a,b) in theList]\n",
    "        if w.lower().startswith(word) )\n",
    "    cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:35:17.842117Z",
     "start_time": "2019-01-11T20:35:17.814427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es lo que descubrimos: <br>\n",
    "TEMA 0- Personas [2004,2009]<br>\n",
    "TEMA 1- Medios de comunicación [2008,2010,2011,2012,2015]<br>\n",
    "TEMA 2- Casciari [2005,2007]<br> \n",
    "TEMA 3- Niñez / Infancia [2006,2013]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T11:40:14.012374Z",
     "start_time": "2019-01-13T11:40:14.002231Z"
    }
   },
   "outputs": [],
   "source": [
    "#Info de Wikipedia\n",
    "casciariTL = {2004:'blog gorda en españa. Nace su hija Nina.',\n",
    "             2005:'premio alemania Deutsche Welle El mejor blog del mundo blog Más respeto, que soy tu madre ',\n",
    "             2006:'Editorial Sudamericana publico en la Argentina y publica Diario de una mujer gorda',\n",
    "             2007:'publicó su segundo libro, España deci alpiste. Colabora El PAis y La Nación',\n",
    "             2008:'Gasalla se interesa por la obra teatro. ',\n",
    "             2009:'se estrena en teatro. Le dió fama y mejora economica. Libro El pibe que arruinaba las fotos',\n",
    "             2010:'renuncia a periódicos y funda Revista Orsai junto a Chiri, amigo de la infancia',\n",
    "             2011:'Aparece primera edición de Orsai. Publica Charlas con mi hemisferio derecho',\n",
    "             2012:'Inicia leyendo cuentos en radio Vorterix, por 2 años',\n",
    "             2013:'Finaliza primera edicion Orsai',\n",
    "             2014:'Edito revista tb para niños Bonsai',\n",
    "             2015:'Publica El nuevo paraíso de los tontos. Se separa de su mujer. Sufre infarto y vuelve a la Argentina'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T11:40:14.577748Z",
     "start_time": "2019-01-13T11:40:14.565866Z"
    }
   },
   "outputs": [],
   "source": [
    "casciariTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones ¿Finales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T16:44:08.172734Z",
     "start_time": "2019-01-08T16:44:08.153858Z"
    }
   },
   "source": [
    "Y ahora... con toda la info obtenida, las gráficas y el TimeLine real de la Vida de Hernan Casciari, a sacar conclusiones! Revisa el artículo sobre NLP en el blog: www.aprendemachinelearning.com "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
